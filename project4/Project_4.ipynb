{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7e2956f-9fb2-45f4-b197-a6f35dfb9a2c",
   "metadata": {},
   "source": [
    "# Introduksjon\n",
    "I dette prosjektet skal vi se på to forskjellige tilnærminger med (maskin læring) praktiske eksempler fra astronomi og helsefag.\n",
    "\n",
    "I topic 1 arbeider vi uten et \"fasit\", vi har fått bilder av melkeveien som vi skulle konvertere til RGB-piksler. Disse verdiene ble brukt for å lage en algortime ved hjelp av K-mean gjennom gruppering og finne mønstre. En algoritmen som oppdager strukutrer basert på likhet. Kan vi automatisk finne mønstre uten at noen forteller oss hva vi skal se?\n",
    "\n",
    "I topic 2 skal vi bruke dataen vi har og lære fra den. I dette tilfelle skal vi finne ut av hvordan Ebola-epidemien utivklet seg i vest afrika ved hjelp av ulike maskinlæringsmodeller. Den gjetter seg frem ved å lære fra historiske data. Vi skal lære hvordan Hvorvidt maskinlæringen kan forutsi ting, hvordan de fungerer praktisk, hva er styrker og svakheter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29668c21",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b017bfa5-3a89-42b4-a24d-e67338cea6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 1: Install the git package and reproduce the image of the milky way via MWSkyMap (see attached code).\n",
    "import numpy as np\n",
    "from astropy import units as u\n",
    "from mw_plot import MWFaceOn\n",
    "\n",
    "mw1 = MWFaceOn(\n",
    "    radius=20 * u.kpc,\n",
    "    unit=u.kpc,\n",
    "    coord=\"galactocentric\",\n",
    "    annotation=True,\n",
    "    figsize=(10, 8),\n",
    ")\n",
    "\n",
    "mw1.title = \"Bird's Eyes View\"\n",
    "\n",
    "mw1.scatter(8 * u.kpc, 0 * u.kpc, c=\"r\", s=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c710345d",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c0b021-ef6e-454d-a364-ca0e6fac00c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 2: Generate a few (at least three in total) visualizations of the milky way sector starting in different centers (try \"M31\") and with different radius (be careful on the units!).\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy import units as u\n",
    "from mw_plot import MWSkyMap\n",
    "\n",
    "mw1 = MWSkyMap(\n",
    "    #center=\"M31\",\n",
    "    radius=(8800, 8800) * u.arcsec,\n",
    "    background=\"Mellinger color optical survey\",\n",
    ")\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "mw1.transform(ax)\n",
    "mw1.savefig('galaxy_large.png')\n",
    "\n",
    "mw2 = MWSkyMap(\n",
    "    #center=\"\",\n",
    "    radius=(5800, 5800) * u.arcsec,\n",
    "    background=\"Mellinger color optical survey\",\n",
    ")\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "mw2.transform(ax)\n",
    "mw2.savefig('galaxy_medium.png')\n",
    "\n",
    "mw3 = MWSkyMap(\n",
    "    #center=\"\",\n",
    "    radius=(2800, 2800) * u.arcsec,\n",
    "    background=\"Mellinger color optical survey\",\n",
    ")\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "mw3.transform(ax)\n",
    "mw3.savefig('galaxy_small.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade1032b",
   "metadata": {},
   "source": [
    "We built three visualizations, with each starting in different centers. We then save each of the figures so that we can use them in later tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de94ae4",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "\n",
    "In this cast we convert the image from task 2 that we chose (galaxy_medium.png) to a numpy array. This numpy array consists of the pixels being constituted as the three colors, red, blue and green. This conversion is done by a function that we made called image_to_rgb_array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e61f41b-76ab-460a-a998-0203beef56ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Convert the image generated into a rgb np.array (each pixel will be a list of 3 number, Red, Green, Blue (rbg).\n",
    "from functions import image_to_rgb_array\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Topic 1 Task 3:\n",
    "def image_to_rgb_array(image_path):\n",
    "    \"\"\"\n",
    "    A function to load an image file and convert it to RGB np.array\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    image_path: str\n",
    "        Path to the image file\n",
    "        \n",
    "    Output\n",
    "    ------\n",
    "    np.array(h, w, 3): A 3d map of each pixel in RGB encoding\n",
    "    \"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    rgb_array = np.array(img)\n",
    "    \n",
    "    # If image has alpha channel (RGBA), take only RGB\n",
    "    if len(rgb_array.shape) == 3 and rgb_array.shape[2] == 4:\n",
    "        rgb_array = rgb_array[:, :, :3]\n",
    "    \n",
    "    return rgb_array\n",
    "\n",
    "# Use the function\n",
    "rgb_array = image_to_rgb_array('galaxy_medium.png')\n",
    "print(f\"Shape: {rgb_array.shape}\")\n",
    "print(f\"Data type: {rgb_array.dtype}\")\n",
    "\n",
    "# Verify it loaded correctly\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(rgb_array)\n",
    "plt.title('Task 3: RGB Array')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb11016",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "\n",
    "Here we want to create an encoding where we manually categorize each pixel to 3 color channels; Red, blue, and green (RGB). Very simple endocing, if red is bigger than blue and green, then the pixel is red-dominant. If blue is bigger than red and green, then blue-dominant etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d696e6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4\n",
    "from functions import categorize_pixels\n",
    "\n",
    "def categorize_pixels(rgb_array):\n",
    "    \"\"\"\n",
    "    Kategoriser piksler som rød, grønn eller blå\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    h, w, _ = rgb_array.shape\n",
    "    categories = np.zeros((h, w), dtype=int)\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            r, g, b = rgb_array[i, j]\n",
    "            if r > g and r > b:\n",
    "                categories[i, j] = 0\n",
    "            elif g > r and g > b:\n",
    "                categories[i, j] = 1\n",
    "            elif b > r and b > g:\n",
    "                categories[i, j] = 2\n",
    "    return categories\n",
    "\n",
    "categories = categorize_pixels(rgb_array)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(categories, cmap='RdYlBu')\n",
    "plt.title('Task 4: Color Dominance Categories\\n(0=Red, 1=Green, 2=Blue)')\n",
    "plt.colorbar(label='Category')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828bf00e",
   "metadata": {},
   "source": [
    "The map looks like this because in the Milky Way there are mostly warmer colors like red, yellow, and orange. There are barely any greens as well, and some stars have a hint of a strong blue in them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fa7396",
   "metadata": {},
   "source": [
    "# Task 5\n",
    "\n",
    "In this task we used K-means clustering. This is a form of an unsupervised machine learning algorithm that groups similar data points by a number of clusters. Relative to this task, the clusters represent the color regions of the map, so the dark regions will clump toger, the brighter regions will clump together etc.. In total there are 6 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb95671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Use K-NN (Nearest neighbor, not discussed in class but extremely simple) or K-means to cluster the data\n",
    "from functions import apply_kmeans_clustering\n",
    "\n",
    "def apply_kmeans_clustering(rgb_array, n_clusters=6, visualize=True):\n",
    "    \"\"\"\n",
    "    Apply K-means clustering to an RGB array\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    rgb_array: np.array\n",
    "        RGB image array (h, w, 3)\n",
    "    n_clusters: int\n",
    "        Number of clusters (default: 6)\n",
    "    visualize: bool\n",
    "        Whether to show the clustering visualization\n",
    "        \n",
    "    Output\n",
    "    ------\n",
    "    predicted_image: np.array\n",
    "        Cluster labels reshaped to image dimensions (h, w)\n",
    "    kmeans: KMeans object\n",
    "        Fitted KMeans model\n",
    "    \"\"\"\n",
    "    from sklearn.cluster import KMeans\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    h, w, _ = rgb_array.shape\n",
    "    rgb_flat = rgb_array.reshape(-1, 3)\n",
    "    rgb_flat = rgb_flat.astype(np.float64)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10, max_iter=300)\n",
    "    kmeans.fit(rgb_flat)\n",
    "    cluster_labels = kmeans.labels_\n",
    "    predicted_image = cluster_labels.reshape(h, w)\n",
    "    \n",
    "    print(f\"Cluster centers:\\n{kmeans.cluster_centers_}\")\n",
    "    print(f\"Unique clusters: {np.unique(cluster_labels)}\")\n",
    "    print(f\"Cluster distribution: {np.bincount(cluster_labels)}\")\n",
    "    \n",
    "    if visualize:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(predicted_image, cmap='viridis')\n",
    "        plt.title(f'K-Means Clustering ({n_clusters} kluster)')\n",
    "        plt.colorbar(label='Cluster ID')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return predicted_image, kmeans\n",
    "\n",
    "# Use the function\n",
    "predicted_image, kmeans = apply_kmeans_clustering(rgb_array, n_clusters=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812a6f38",
   "metadata": {},
   "source": [
    "# Task 6\n",
    "\n",
    "We imposed the cluster to the image back in task 2, specifically we chose the galaxy_medium.png. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042fd574-ae33-4619-88e4-130e283316b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6: Over-impose your cluster to the image generated in task 2\n",
    "import matplotlib.pyplot as plt\n",
    "from functions import image_to_rgb_array\n",
    "\n",
    "\n",
    "# We reuse the function from task 3 to grab the picture\n",
    "rgb_array_task6 = image_to_rgb_array('galaxy_medium.png')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(rgb_array_task6)\n",
    "axes[0].set_title('Original Milky Way', fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Overlay\n",
    "axes[1].imshow(rgb_array_task6)\n",
    "axes[1].imshow(predicted_image, cmap='viridis', alpha=0.5)\n",
    "axes[1].set_title('Task 6: Cluster Overlay', fontsize=14, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.colorbar(axes[1].images[1], ax=axes[1], label='Cluster ID', fraction=0.046)\n",
    "plt.tight_layout()\n",
    "plt.savefig('task6_cluster_overlay.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c9b094",
   "metadata": {},
   "source": [
    "We show the side by side between the original Milky Way and the one with the clustered overlay, the higher the alpha, the higher the overlay will show. We see a clear distinction between the darker green at the top and the yellowish-green at the bottom. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a479fc3b",
   "metadata": {},
   "source": [
    "# Task 7\n",
    "\n",
    "In this task we want to try different methods to categorize the pixels. In task 4 we used the colors red, blue and green. This time we will take a different approach, in which we will be using the brightness instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29445e1-c5e1-4e5b-800e-f16f099d090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7: Try different categories (task 4), repeating tasks 5 and 6\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from functions import apply_kmeans_clustering, image_to_rgb_array\n",
    "\n",
    "# Load image with function from task 3\n",
    "rgb_array = image_to_rgb_array('galaxy_medium.png')\n",
    "\n",
    "# Task 4 but based on brightness\n",
    "def categorize_pixels_brightness(rgb_array):\n",
    "    \"\"\"\n",
    "    Categorizes pixels based on brightness\n",
    "    \"\"\"\n",
    "    h, w, _ = rgb_array.shape\n",
    "    categories = np.zeros((h, w), dtype=int)\n",
    "    brightness = rgb_array.mean(axis=2)\n",
    "    \n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            if brightness[i, j] < 85:\n",
    "                categories[i, j] = 0  # Mørk\n",
    "            elif brightness[i, j] < 170:\n",
    "                categories[i, j] = 1  # Medium\n",
    "            else:\n",
    "                categories[i, j] = 2  # Lys\n",
    "    return categories\n",
    "\n",
    "categories = categorize_pixels_brightness(rgb_array)\n",
    "print(f\"Kategori-shape: {categories.shape}\")\n",
    "\n",
    "# Visualize the categories\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(categories, cmap='viridis')\n",
    "plt.title('Task 4: Brightness Categories (0=Dark, 1=Medium, 2=Bright)')\n",
    "plt.colorbar(label='Category')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# We use the function from task 5\n",
    "predicted_image, kmeans = apply_kmeans_clustering(rgb_array, n_clusters=6, visualize=True)\n",
    "\n",
    "# Task 6: Over-impose cluster to the image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(rgb_array)\n",
    "plt.imshow(predicted_image, cmap='viridis', alpha=0.5)\n",
    "plt.title('Task 6: Cluster Overlay on Milky Way', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(label='Cluster ID')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('task7_cluster_overlay.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3810d1f5",
   "metadata": {},
   "source": [
    "In the first image it is a visualization of task 4 where we now based it on brightness. By comparing it to the first encoding where we based it on colors, this one in task 7 seems much better. The previous had too much red and felt like it wasn't representative of the mat itself. Meanwhile the one based on brightness feels better.\n",
    "\n",
    "Between our task 4 encoding versus the K-means cluster method, we can see much more differences. K-means found much more patterns that we didn't program, for example in task 4 brightness method (between -0.5 and 0.0), that is the only bright spot apparently, but in the K-means, it automatically identified the dark space, and galactic center."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2822698b",
   "metadata": {},
   "source": [
    "# Topic 1 Conclusion\n",
    "\n",
    "To summarize the topic, first we had to install a python library where we could then reproduce an image of the Milky Way Galaxy. We then created some visualizations. After the visualizations, we encoded some methods to that we could segment each region og the milky way, so that we could see some recognizable patterns. \n",
    "\n",
    "We encoded a manual method where we first based the milky way of the three colors red, blue, and green. We found quickly out that it was a bad method as the result was simply not detailed. After that we used the K-means clustering where we had much more success. It groups similar pixels into clusters. This method recognized immidiately the characteristics of the milky way, such as bright center, and the dark space.\n",
    "\n",
    "We conclude that unsupervised machine learning can be much more effective in identifying patterns and segment structures of the galaxy, instead of human rules. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11467079",
   "metadata": {},
   "source": [
    "# Topic 2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8175af",
   "metadata": {},
   "source": [
    "# MOD300 Project 4 - Task 0: Reproduce Ebola SEZR Model Plots\n",
    "\n",
    "Reproducing the SEZR (Susceptible-Exposed-Infected-Removed) model analysis from Project 2, Exercise 5 for the West Africa Ebola outbreak in Guinea, Liberia, and Sierra Leone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9034727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from functions import load_ebola_data, solve_sezr, plot_ebola_data, plot_model_comparison\n",
    "\n",
    "# Load data\n",
    "countries = {'Guinea': 'ebola_cases_guinea.dat', 'Liberia': 'ebola_cases_liberia.dat', \n",
    "             'Sierra Leone': 'ebola_cases_sierra_leone.dat'}\n",
    "ebola_data = {c: dict(zip(['days', 'new_cases', 'cumulative'], load_ebola_data(f))) \n",
    "              for c, f in countries.items()}\n",
    "\n",
    "# Plot real data\n",
    "for country, data in ebola_data.items():\n",
    "    plot_ebola_data(data['days'], data['new_cases'], data['cumulative'], country)\n",
    "    plt.show()\n",
    "\n",
    "# Fit SEZR model and compare\n",
    "N = 1e7\n",
    "model_params = {'Guinea': {'beta0': 0.6, 'lam': 0.0012}, \n",
    "                'Liberia': {'beta0': 0.8, 'lam': 0.0015},\n",
    "                'Sierra Leone': {'beta0': 0.7, 'lam': 0.0010}}\n",
    "\n",
    "for country, data in ebola_data.items():\n",
    "    params = model_params[country]\n",
    "    t, solution = solve_sezr(params['beta0'], params['lam'])\n",
    "    fig, axes, r2 = plot_model_comparison(data['days'], data['new_cases'], data['cumulative'], \n",
    "                                          t, solution, params['beta0'], params['lam'], country, N)\n",
    "    plt.show()\n",
    "    print(f\"{country}: β₀={params['beta0']:.4f}, λ={params['lam']:.6f}, R²={r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a312dc47",
   "metadata": {},
   "source": [
    "# MOD300 Project 4 - Task 1: Train a line with linear regression on the data for the three countries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606d4fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from functions import train_linear_regression, plot_linear_regression\n",
    "\n",
    "print(\"\\nLinear Regression on Ebola Data\\n\")\n",
    "\n",
    "linear_results = {}\n",
    "for country, data in ebola_data.items():\n",
    "\n",
    "    model, predictions, metrics = train_linear_regression(data['days'], data['cumulative'])\n",
    "    linear_results[country] = {'model': model, 'predictions': predictions, 'metrics': metrics}\n",
    "\n",
    "    print(f\"{country} Linear Regression:\")\n",
    "    print(f\" Equation: y = {metrics['slope']:.2f}x + {metrics['intercept']:.0f}\")\n",
    "    print(f\" R²: {metrics['r2']:.3f}, RMSE: {metrics['rmse']:.2f}\\n\")\n",
    "\n",
    "print(\"Plotting Linear Regression Results...\\n\")\n",
    "for country, data in ebola_data.items():\n",
    "    result = linear_results[country]\n",
    "    plot_linear_regression(data['days'], data['new_cases'], data['cumulative'], result['predictions'],result['metrics'], country)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9c823c",
   "metadata": {},
   "source": [
    "# MOD300 Project 4 - Task 2: Train a better fitting function than a single line with linear regression on the data for the three countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51337c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import plot_polynomial_comparison, plot_polynomial_regression, compare_polynomial_degrees, plot_model_comparison\n",
    "print(\"TASK 2: Polynomial Regression Analysis\")\n",
    "\n",
    "degrees_to_test = [2, 3, 4, 5]\n",
    "polynomial_results = {}\n",
    "for country, data in ebola_data.items():\n",
    "    print(f\"\\nAnalyzing {country} Data:\")\n",
    "    results, best_degree = compare_polynomial_degrees(data['days'], data['cumulative'], degrees_to_test)\n",
    "    polynomial_results[country] = {'results': results, 'best_degree': best_degree}\n",
    "    \n",
    "    for degree in sorted(results.keys()):\n",
    "        metrics = results[degree]['metrics']\n",
    "        print(f\" Degree {degree}: R² = {metrics['r2']:.3f}, RMSE = {metrics['rmse']:.2f}\")\n",
    "    print(f\" Best Degree for {country}: {best_degree}\\n\")\n",
    "\n",
    "print(\"Plotting Polynomial Regression Results...\\n\")\n",
    "for country, data in ebola_data.items():\n",
    "    best_degree = polynomial_results[country]['best_degree']\n",
    "    result = polynomial_results[country]['results'][best_degree]\n",
    "\n",
    "    plot_polynomial_regression(\n",
    "        data['days'],\n",
    "        data['new_cases'],\n",
    "        data['cumulative'],\n",
    "        result['predictions'],\n",
    "        result['metrics'],\n",
    "        best_degree,\n",
    "        country\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot comparison of all degrees for each country\n",
    "print(\"\\nPlotting degree comparisons...\")\n",
    "for country, data in ebola_data.items():\n",
    "    plot_polynomial_comparison(\n",
    "        data['days'],\n",
    "        data['cumulative'],\n",
    "        polynomial_results[country]['results'],\n",
    "        country\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d52df0",
   "metadata": {},
   "source": [
    "# MOD300 Project 4 - Task 3: Train a NN network and predict the epidemic evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7c24a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import temporal_train_test_split, plot_NN_network_results, compare_NN_architectures\n",
    "\n",
    "print(\"TASK 3: Neural Network Regression Analysis\")\n",
    "nn_results = {}\n",
    "architectures = [(50,), (100,), (100, 50), (100, 50, 25)]\n",
    "\n",
    "for country, data in ebola_data.items():\n",
    "    print(f\"\\nAnalyzing {country} Data:\")\n",
    "    try:\n",
    "        # Temporal split\n",
    "        X_train, X_test, y_train, y_test, split_idx = temporal_train_test_split(\n",
    "            data['days'], \n",
    "            data['cumulative'], \n",
    "            train_ratio=0.7\n",
    "        )\n",
    "        print(\" Split successful\")\n",
    "        \n",
    "        # Print split info\n",
    "        print(f\"  Train size: {len(X_train)} points (days 0-{data['days'][split_idx-1]})\")\n",
    "        print(f\"  Test size: {len(X_test)} points (days {data['days'][split_idx]}-{data['days'][-1]})\")\n",
    "        \n",
    "        # Compare architectures\n",
    "        results, best_arch = compare_NN_architectures(\n",
    "            X_train, y_train, X_test, y_test, \n",
    "            architectures=architectures\n",
    "        )\n",
    "        \n",
    "        # Print architecture comparison\n",
    "        print(\"  Architecture Comparison:\")\n",
    "        for arch, result in results.items():\n",
    "            metrics = result['metrics']\n",
    "            print(f\"    {arch}: Train R²={metrics['train_r2']:.3f}, \"\n",
    "                  f\"Test R²={metrics['test_r2']:.3f}\")\n",
    "        \n",
    "        print(f\"  ✓ Best architecture: {best_arch}\")\n",
    "        \n",
    "        # Store results\n",
    "        nn_results[country] = {\n",
    "            'results': results,\n",
    "            'best_arch': best_arch,\n",
    "            'split_idx': split_idx\n",
    "        }\n",
    "        \n",
    "        print(f\"  ✓ {country} completed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ ERROR for {country}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Plotting loop (separate)\n",
    "print(\"\\nPlotting Neural Network Results...\")\n",
    "for country, data in ebola_data.items():\n",
    "    if country not in nn_results:\n",
    "        print(f\"⚠️ Skipping {country} - no results available\")\n",
    "        continue\n",
    "    \n",
    "    best_arch = nn_results[country]['best_arch']\n",
    "    result = nn_results[country]['results'][best_arch]\n",
    "    split_idx = nn_results[country]['split_idx']\n",
    "    \n",
    "    plot_NN_network_results(\n",
    "        data['days'],\n",
    "        data['cumulative'],\n",
    "        split_idx,\n",
    "        result['train_predictions'],\n",
    "        result['test_predictions'],\n",
    "        result['metrics'],\n",
    "        country\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875f7e25",
   "metadata": {},
   "source": [
    "# Task 4: Train a LSTM (a NN specialized for time series) and predict the epidemic evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31edf501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import ( plot_lstm_results, compare_lstm_configurations)\n",
    "\n",
    "# Check if TensorFlow is available\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print(f\"✓ TensorFlow {tf.__version__} is available\")\n",
    "    print(\"  LSTM Task 4 can proceed!\\n\")\n",
    "except ImportError:\n",
    "    print(\"✗ TensorFlow not installed!\")\n",
    "    print(\"  Install with: pip install tensorflow\")\n",
    "    print(\"  Skipping Task 4...\\n\")\n",
    "    raise\n",
    "\n",
    "print(\"TASK 4: LSTM Network Analysis\")\n",
    "# LSTM configurations to test\n",
    "lstm_configs = [\n",
    "    {'lookback': 7, 'units': 50, 'layers': 1, 'dropout': 0.2},   # Simple: 1 week lookback\n",
    "    {'lookback': 14, 'units': 50, 'layers': 2, 'dropout': 0.2},  # Medium: 2 weeks\n",
    "    {'lookback': 21, 'units': 100, 'layers': 2, 'dropout': 0.2}, # Complex: 3 weeks\n",
    "]\n",
    "\n",
    "lstm_results = {}\n",
    "for country, data in ebola_data.items():\n",
    "    print(f\"\\n{country}:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Compare LSTM configurations\n",
    "        results, best_config = compare_lstm_configurations(\n",
    "            data['days'],\n",
    "            data['cumulative'],\n",
    "            configurations=lstm_configs,\n",
    "            train_ratio=0.7,\n",
    "            epochs=100,\n",
    "            verbose=0  \n",
    "        )\n",
    "        \n",
    "        # Print results\n",
    "        print(\" LSTM Configuration Comparison:\")\n",
    "        for config_name, result in results.items():\n",
    "            metrics = result['metrics']\n",
    "            print(f\"    {config_name}: \"\n",
    "                  f\"Train R²={metrics['train_r2']:.3f}, \"\n",
    "                  f\"Test R²={metrics['test_r2']:.3f}, \"\n",
    "                  f\"Epochs={metrics['epochs_trained']}\")\n",
    "        \n",
    "        print(f\"  ✓ Best configuration: {best_config}\")\n",
    "        \n",
    "        lstm_results[country] = {\n",
    "            'results': results,\n",
    "            'best_config': best_config\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Plot LSTM results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Plotting LSTM Results (Best Configuration)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "for country, data in ebola_data.items():\n",
    "    if country not in lstm_results:\n",
    "        print(f\"⚠️ Skipping {country} - no results\")\n",
    "        continue\n",
    "    \n",
    "    best_config = lstm_results[country]['best_config']\n",
    "    result = lstm_results[country]['results'][best_config]\n",
    "    \n",
    "    plot_lstm_results(\n",
    "        data['days'],\n",
    "        data['cumulative'],\n",
    "        result['config']['lookback'],\n",
    "        result['train_size'],\n",
    "        result['scaler'],\n",
    "        result['train_predictions'],\n",
    "        result['test_predictions'],\n",
    "        result['metrics'],\n",
    "        country\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca36779",
   "metadata": {},
   "source": [
    "# Task 5: Discuss the results. \n",
    "\n",
    "# Can we ignore modeling and let Machine Leaning make prediction?\n",
    "No. We cannot ignore traditional modeling for three important reasons.\n",
    "First, SEZR tells us WHY things happen. The model parameters have real meaning: β₀ shows how fast the disease spreads, λ shows how quickly people change behavior, σ is the incubation time, and γ is recovery time. For example, when β₀=0.8 for Liberia, we know transmission is high and need strong interventions. Machine learning models don't give us these useful insights - knowing a neural network has 100 neurons doesn't help us decide what to do.\n",
    "Second, SEZR can test \"what if\" questions. We can ask \"what happens if we reduce transmission by 50%?\" and immediately see the impact of a lockdown. Machine learning can't do this - we'd have to retrain the entire model every time we want to test a new scenario. This makes ML too slow for quick decision-making during an outbreak.\n",
    "Third, our results show ML can fail badly. Polynomial regression worked consistently well (R²=0.89-0.92 for all countries). But LSTM completely failed for Liberia - it got R²=0.88 on training data but only R²=0.27 on test data. This huge gap (0.61) means the model just memorized the training data instead of learning real patterns. Meanwhile, SEZR gave consistent R²≈0.90 for all countries.\n",
    "However, ML is still useful for finding patterns and checking our models. The best approach uses both: SEZR for understanding the disease and making policy decisions, plus ML (especially polynomial regression) to verify accuracy.\n",
    "\n",
    "# What is a good prediction for these cases? \n",
    "A good prediction needs more than just high accuracy.\n",
    "Accuracy (R² should be at least 0.85): SEZR (0.90), polynomial (0.91), and neural networks (0.88) all did great. LSTM was unreliable - it worked well for Guinea (0.94) and Sierra Leone (0.85) but completely failed for Liberia (0.27). This inconsistency makes LSTM unacceptable for public health.\n",
    "\"Can we understand and use it?\", the clear winner SEZR is winner because its parameters tell us exactly what to do: β₀ shows how much to reduce contact, λ shows if awareness campaigns work, and σ and γ tell us how long to quarantine people. Polynomial regression is somewhat understandable (we can see the curve shape). LSTM is a black box - we can't explain its predictions to health officials or the public.\n",
    "Reliability means \"does it work consistently?\" SEZR and polynomial were very consistent - their R² only varied by 0.03 across the three countries. LSTM varied wildly (from 0.27 to 0.94), making it unreliable for systematic use.\n",
    "Predicting the future requires the model to work well on new data. We measure this by comparing training R² to test R². Neural networks did well (gaps of 0.07-0.08). LSTM worked for Guinea (gap=0.05) but failed badly for Liberia (gap=0.61), meaning it just memorized training data instead of learning real patterns.\n",
    "Data requirements matter for early outbreaks. SEZR only needs 20-30 data points because it uses our knowledge of how diseases spread. Polynomial needs 50+, neural networks need 100+, and LSTM needs 200+. This makes SEZR essential when an outbreak starts and we have little data.\n",
    "Recommendations: For policy decisions, use SEZR (understandable and useful). For accurate predictions with enough data, use polynomial regression (R²=0.91, reliable). If you don't have disease experts, use polynomial or neural networks. Avoid linear regression (too simple, R²=0.79) and LSTM (needs too much data, inconsistent results).\n",
    "\n",
    "# Conclusion.\n",
    "- Finding 1: Simpler models often work better. Simple polynomial regression (R²=0.91) beat the complex LSTM model (R²=0.69). For Liberia, polynomial got R²=0.915 while LSTM only got 0.270. This shows that more complicated doesn't mean better. The lesson: use the simplest model that works well.\n",
    "\n",
    "- Finding 2: Machine learning can't replace understanding the disease. Polynomial regression was just as accurate as SEZR (0.91 vs 0.90), but it can't tell us how the disease spreads, can't test \"what if\" scenarios, and won't work for different diseases. SEZR gives us the understanding we need to make decisions and save lives.\n",
    "\n",
    "- Finding 3: We must test models on future data. The Liberia LSTM looked good on training data (R²=0.88) but failed on test data (R²=0.27). This means it memorized the training data instead of learning real patterns. We caught this by testing the model on later days (days 70-100) that it hadn't seen during training. This type of testing is essential for time series data.\n",
    "\n",
    "- Finding 4: Different stages need different models. Early in an outbreak (days 1-30), use SEZR because we have little data. In the middle (days 30-200), use SEZR plus polynomial to double-check. Late in the outbreak (200+ days), we can use polynomial or neural networks for forecasting. One model can't do everything.\n",
    "\n",
    "- Finding 5: Complex models need lots of data. LSTM failed for Liberia because it was too complex for the amount of data we had (~200 data points, reduced to ~140 by the lookback window). Polynomial regression worked because it's simpler and needs less data.\n",
    "\n",
    "- Finding 6: The gap between training and test scores reveals problems. Small gaps (<0.10) are good - the model learned real patterns. Large gaps (>0.20) are bad - the model just memorized. Liberia LSTM had a huge gap (0.61), warning us not to trust it.\n",
    "\n",
    "- What to do: (1) Use SEZR as your main tool for understanding and making decisions. (2) Use polynomial regression to check accuracy. (3) Always compare complex models to simple ones. (4) Test on future data, not random data. (5) Choose models you can explain to decision-makers.\n",
    "- Limitations: We only studied three countries from one Ebola outbreak and didn't try all possible LSTM settings. Future studies should test on different diseases (COVID-19, flu), combine SEZR with ML in new ways, and measure prediction uncertainty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1442c2",
   "metadata": {},
   "source": [
    "# Self-reflections\n",
    "\n",
    "Rahul: For my part, I found the project very interesting and meaningful. It was very cool to see how something that has been studied for so many years can be visualized with unsupervised machine learning-methods. The difference between automatic and manual computing was massive, and there were a lot of things that we could take away from that. We worked mostly individually with frequent communications between us. The tasks were on a decent level as in some places we needed help and others we breezed past them. AI was used to start the K-means clusters task as I had no idea how to move forward with it. \n",
    "\n",
    "Sem: I dette prosjektet fikk jeg lært me g hvordan man kan finne mønstre i data uten fasit ved å bruke K-means. Algoritmen grupperer basert på RGB-likhet (konverter bilde til RGB-piksler) for å finne ut hvordan bilde av melkevei skal se ut, altså strukturen. For min del var det ikke helt åpenbart hvilke farger som skulle bli satt i fokus eller hvor mange grupper som gir mest mening, ettersom jeg fikk forskjellige resultater med ulike inputs. \n",
    "\n",
    "Eric: I learned how to use supervised learning using historical data to predict the future. That's exactly what I did by training different models (linear regression, polynomial, neural network, and LSTM) on the Ebola data to compare how well the models are able to predict the development. You quickly see that none of the models are perfect, and all have their limitations. You have to see where it is sufficient to use it for which models.\n",
    "\n",
    "**Task Distribution**\n",
    "Since the topic 2 in this project was relevant to project 2 that we did together, it made it easier for us to distribute the tasks.\n",
    "\n",
    "Sem: Topic 1 - Task 0, 1, 2, 3, 4\n",
    "\n",
    "Rahul: Topic 1 - Task 5, 6, 7\n",
    "\n",
    "Eric: Topic 2 - Task 0, 1, 2, 3, 4, 5\n",
    "\n",
    "\n",
    "I denne oppgaven så har vi brukt Ai som en lærlingsverktøy for å få en bedre forståelse av prinsippene av oppgavene.  Topic 1 for å forstå hvordan K-means fungerer og hvordan vi kan implementere dette i oppgaven, ettersom dette ikke hadde blitt diskutert i klassen. Topic 2 ble ai brukt for å forstå hvordan de forskjellige modellene fungerer og få en bedre forståelse for konseptene. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac5cde1",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "<a id=\"ref1\"></a>Brownlee, J. (n.d.). *Time series prediction with LSTM recurrent neural networks in Python with Keras*. Machine Learning Mastery. https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "<a id=\"ref2\"></a>Claude AI by Anthropic. (2024). https://claude.ai\n",
    "\n",
    "<a id=\"ref3\"></a>MOD300 Project 2, Exercise 5. (2024). *Ebola outbreak data and SEZR model implementation*.\n",
    "\n",
    "<a id=\"ref4\"></a>Aksel Hiorth. *Computational Engineering and Modeling*. https://github.com/ahiorth/CompEngineering, 2021.\n",
    "\n",
    "<a id=\"ref5\"></a>Ringnes, Truls; Hammerstrøm, Maria: *Melkeveien i Store norske leksikon på snl.no.* Hentet 4. desember 2025 fra https://snl.no/Melkeveien.\n",
    "\n",
    "<a id=\"ref6\"></a>GeeksforGeeks. (2023, June 22). *K-means clustering – Introduction. GeeksforGeeks*. https://www.geeksforgeeks.org/machine-learning/k-means-clustering-introduction/\n",
    "\n",
    "<a id=\"ref7\"></a>W3Schools. (n.d.). *Python Machine Learning – K-means.* https://www.w3schools.com/python/python_ml_k-means.asp\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
